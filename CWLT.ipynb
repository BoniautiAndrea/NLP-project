{"metadata":{"colab":{"provenance":[],"collapsed_sections":["1NOYAGsUkNcw","RQAMip2ZkeSi","YJkQ13rjmeNX","CRtCebXW5Jn3","LXD4eQYenZs2","DAbi7TvkzJyB"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# C-WLT\n","\n","This paper is the reimplementation of 'Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models' from https://arxiv.org/pdf/2304.13803.pdf with some improvements, more details can be found in the Report delivered with this notebook.\n","\n","All cells before Dataset Run are required to run if the user wants to use the Dataset run (it may take around 30/40 minutes if using bloom-3b)\n","\n","For the the Example run, the Data Loading section can be skipped, but as explained in the specific section, it uses BabelNet so it is incompatible with Colab, you will need to download the notebook and its Google Drive folder (or just create an examples/ folder in the same place). Also be sure to have a BabelNet API key, if not, one comes with the folder.\n","\n","This project is delivered with original samples data (in data/ folder) and with already sampled data (in samples/ folder), the code automatically check if already sampled data exists and if not it loads original data and samples it.\n","\n","If you want to try the sampling procedure just empty the /samples folder in the Google Drive repository.\n","\n","The Dataset run is based on some XL-WSD files and on the 'best-ensemble settings' as said in the paper, in which the language of the prompt is in English and the target languages (the ones in which the words are translated) are English, Russian and Chinese. If one of the target languages is selected as source language, it will just not be translated in the same one.\n","\n","To try the code with a new {new_lang}, you will need to:\n","Download from https://github.com/mk322/contextual-word-level-translation the following files:\n","1. test-{new_lang}.data.xml and test-{new_lang}.gold.key.txt from xl-wsd-data/evaluation_datasets/test-{new_lang}\n","2. correct_trans_{new_lang}_{tlang}.json, wrong_trans_{new_lang}_{tlang}.json and all_sense_labels_{new_lang}_{tlang}.txt from xl-wsd-files/{new_lang}/, where {tlang} is 'en', 'ru' and 'zh'.\n","Add these files to data/ folder and write as parameter of wsd_on_dataset() function the iso of the new language (es. 'es' for spanish)\n","\n","These files were created by paper authors, they allow the run without using BabelNet, incompatible with Colab environment.\n"],"metadata":{"id":"5FpK9EuvjuJ_"}},{"cell_type":"markdown","source":["# Folder Setup, Imports etc.\n","READ BEFORE RUN!\n","Few important things.:\n","Run the first cell if using GPU, some new update of Colab made the models using too much GPU and this can exceed the GPU RAM with even affordable models.\n","\n","There are 2 ways of loading data:\n","1. Through gdown, this is nice as it does not requires interactions with popups and permission requests, but the downside is that it dowsn't allow to actually write and save file in the real Drive folder, all generated files will be stored locally on the notebook runtime and you will need to open through it or download them by your own.\n","\n","2. With the official mounting of the drive given by Google. With this you can save files in the real folder, but each time you start the notebook you will have to interact with the popup.\n","\n","The second modality is actually commented in the 3rd cell."],"metadata":{"id":"1NOYAGsUkNcw"}},{"cell_type":"code","source":["# RUN THIS CELL IF USING GPU\n","!wget http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/google-perftools_2.5-2.2ubuntu3_all.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n","!apt install -qq libunwind8-dev\n","!dpkg -i *.deb\n","%env LD_PRELOAD=libtcmalloc.so"],"metadata":{"id":"iWlZDmr28zIz","execution":{"iopub.status.busy":"2024-03-18T15:27:32.828515Z","iopub.execute_input":"2024-03-18T15:27:32.829128Z","iopub.status.idle":"2024-03-18T15:27:49.148407Z","shell.execute_reply.started":"2024-03-18T15:27:32.829076Z","shell.execute_reply":"2024-03-18T15:27:49.147160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GDOWN LOADING\n","\n","!pip install -q accelerate gdown\n","#Loading the Drive folder with the required data\n","import gdown\n","url = \"https://drive.google.com/drive/folders/1DaBLm4BHApfnfBARHusaEZEPo0BCnnWO?usp=sharing\"\n","gdown.download_folder(url, quiet=True, use_cookies=False)\n","\n","%cd HotNLP_BonaiutiAndrea/"],"metadata":{"id":"sdOjtsDFjkFa","execution":{"iopub.status.busy":"2024-03-18T15:27:49.150735Z","iopub.execute_input":"2024-03-18T15:27:49.151091Z","iopub.status.idle":"2024-03-18T15:28:21.209383Z","shell.execute_reply.started":"2024-03-18T15:27:49.151061Z","shell.execute_reply":"2024-03-18T15:28:21.208338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OFFICIAL COLAB LOADING\n","\n","#!pip install -q accelerate gdown\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#%cd drive/MyDrive/HotNLP_BonaiutiAndrea/"],"metadata":{"id":"WbKK5jScmj0E","execution":{"iopub.status.busy":"2024-03-18T15:28:21.210496Z","iopub.execute_input":"2024-03-18T15:28:21.210884Z","iopub.status.idle":"2024-03-18T15:28:22.229545Z","shell.execute_reply.started":"2024-03-18T15:28:21.210858Z","shell.execute_reply":"2024-03-18T15:28:22.228545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# All needed libraries\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import json\n","import xml.etree.ElementTree as ET\n","import torch.nn.functional as F\n","import torch\n","from typing import List\n","import os\n","import pickle\n","from tqdm.notebook import tqdm\n","import math"],"metadata":{"id":"EzcU4-CilDQ4","execution":{"iopub.status.busy":"2024-03-18T15:28:22.231116Z","iopub.execute_input":"2024-03-18T15:28:22.231440Z","iopub.status.idle":"2024-03-18T15:28:32.312083Z","shell.execute_reply.started":"2024-03-18T15:28:22.231414Z","shell.execute_reply":"2024-03-18T15:28:32.311040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility functions and some 'global variables'"],"metadata":{"id":"RQAMip2ZkeSi"}},{"cell_type":"code","source":["# if case you forget to define target languages\n","TARGET_LANGS = ['en', 'zh', 'ru']\n","\n","# dict to translate language words\n","LANG_DICT = {'ru' : 'Russian', 'zh' : 'Chinese', 'en' : 'English', 'es': 'Spanish', 'fr' : 'French',\n","             'bg' : 'Bulgarian', 'ca' : 'Catalan', 'da' : 'Danish', 'de' : 'German', 'et': 'Estonian',\n","             'eu' : 'Basque', 'gl' : 'Galician', 'hr' : 'Croatian', 'hu' : 'Hungarian', 'ja' : 'Japanese'\n","             'nl' : 'Dutch', 'sl' : 'Slovenian'}\n","\n","# check/modify use of gpu/cpu\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Computations will be done on: {device}')"],"metadata":{"id":"WfQ2pMO0kmf-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710760352457,"user_tz":-60,"elapsed":11,"user":{"displayName":"Andrea Bonaiuti","userId":"11668450428526688147"}},"outputId":"38beb315-3bf6-4aa1-fdf5-93821869befa","execution":{"iopub.status.busy":"2024-03-18T15:28:32.315212Z","iopub.execute_input":"2024-03-18T15:28:32.315698Z","iopub.status.idle":"2024-03-18T15:28:32.373556Z","shell.execute_reply.started":"2024-03-18T15:28:32.315671Z","shell.execute_reply":"2024-03-18T15:28:32.372379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Computations will be done on: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":["# word object\n","# it contains for each word, its label (correct sense), and all candidate synsets and translations for all target languages\n","class Word:\n","    def __init__(self, text, pos, translations, labels = None):\n","        self.text = text\n","        self.pos = pos\n","        self.translations = translations\n","        self.labels = labels\n","\n","# dataset sample object\n","class WsdSample:\n","    def __init__(self, id = None, sentence = None, words : List[Word] = None):\n","        self.id = id\n","        self.sentence = sentence\n","        # list of words to be translated\n","        self.words = words\n","\n","# define data path\n","def data_paths(lang):\n","    sentence_path = f'data/test-{lang}.data.xml'\n","    label_path = f'data/test-{lang}.gold.key.txt'\n","    return sentence_path, label_path\n","\n","# parsing utility of lines in source file\n","def dict_from_lines(lines):\n","    d = {}\n","    for line in lines:\n","        line = line.replace('\\n', '')\n","        sep = line.find('bn:')\n","        word = line[:(sep - 1)]\n","        labels = line[sep:].split(' ')\n","        d[word] = labels\n","    return d\n","\n","# building dicts to retrieve at inference time all senses from translations (used to avoid BabelNet)\n","def build_id_dicts(source_lang, target_langs = TARGET_LANGS):\n","    print('Building useful dictionaries..')\n","    dicts = {}\n","\n","    for lang in target_langs:\n","        input_path = f'data/all_sense_labels_{source_lang}_{lang}.txt'\n","        out_path = f'dicts/senses_{lang}_for_{source_lang}_data.json'\n","\n","        if os.path.exists(out_path):\n","            with open(out_path, 'r', encoding='utf-8') as f:\n","                d = json.load(f)\n","        else:\n","            d = {}\n","        with open(input_path, 'r', encoding='utf-8') as f:\n","            lines = f.readlines()\n","            d = dict_from_lines(lines)\n","        with open(out_path, 'w', encoding='utf-8') as f:\n","            json.dump(d, f, indent=4)\n","        dicts[lang] = d\n","\n","    print('Done')\n","    return dicts\n","\n","# building dicts with all possible translations for source words, used to avoid BabelNet in preprocessing phase\n","def build_vocabs(source_lang, target_langs = TARGET_LANGS):\n","    print('Building vocabs..')\n","    vocabs = {}\n","\n","    for lang in target_langs:\n","        correct_path = f'data/correct_trans_{source_lang}_{lang}.json'\n","        wrong_path = f'data/wrong_trans_{source_lang}_{lang}.json'\n","        out_path = f'dicts/vocab_{lang}.json'\n","\n","        if os.path.exists(out_path):\n","            with open(out_path, 'r', encoding='utf-8') as file:\n","                vocab = json.load(file)\n","        else:\n","            vocab = {}\n","            with open(correct_path) as file:\n","                vocab = json.load(file)\n","            with open(wrong_path) as file:\n","                wrong = json.load(file)\n","            for key in wrong.keys():\n","                vocab[key].extend(wrong[key])\n","            with open(out_path, 'w', encoding='utf-8') as file:\n","                json.dump(vocab, file, indent=4)\n","        vocabs[lang] = vocab\n","\n","    print('Done')\n","    return vocabs\n","\n","# function to create the PLM prompt\n","def get_prompt(sentence, word, target_lang):\n","    return f\"In the sentence \\\" {sentence} \\\", the word {word} is translated into {LANG_DICT[target_lang]} as \\\"\"\n","\n","# used in the alternative method of translating, to compute the minimum number of different tokens we need to mask the words\n","# this method takes all the words in tokens and builds a mask with the minimum dfferent initial tokens for each word\n","def get_tokens_mask(tokens):\n","    mask = []\n","    for i in range(len(tokens)):\n","        mask.append([])\n","    max_length = 0\n","    for i in range(len(tokens)):\n","        max_length = len(tokens[i]) if len(tokens[i]) > max_length else max_length\n","    tensor = torch.zeros((len(tokens), max_length), dtype=torch.int32)\n","    for i, word in enumerate(tokens):\n","        for j, token in enumerate(word):\n","            tensor[i,j] = token\n","    indices = []\n","    for i in range(max_length):\n","        _, inverse, counts = torch.unique(tensor[:,i], return_inverse=True, return_counts=True)\n","        if torch.equal(torch.ones(counts.size()), counts):\n","            return mask\n","        else:\n","            duplicates = [torch.where(inverse == i)[0] for i, c, in enumerate(counts) if counts[i] > 1]\n","            if i == 0:\n","                for dup in duplicates:\n","                    for elem in dup:\n","                        indices.append(elem.tolist())\n","            else:\n","                temp = indices\n","                indices = []\n","                for dup in duplicates:\n","                    for elem in dup:\n","                        if elem in temp:\n","                            indices.append(elem.tolist())\n","            if len(indices) > 1:\n","                for ind in indices:\n","                    mask[ind].append(1)\n","    return mask\n","\n","# used to truncate some decimals and see if more than 1 results has very same, if not equal, score\n","def truncate(f, n):\n","    if f > 0:\n","        return math.floor(f * 10 ** n) / 10 ** n\n","    else:\n","        return math.ceil(f * 10 ** n) / 10 ** n"],"metadata":{"id":"D1PRBHoDmCVT","execution":{"iopub.status.busy":"2024-03-18T15:28:32.375164Z","iopub.execute_input":"2024-03-18T15:28:32.375574Z","iopub.status.idle":"2024-03-18T15:28:32.402945Z","shell.execute_reply.started":"2024-03-18T15:28:32.375541Z","shell.execute_reply":"2024-03-18T15:28:32.402031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Loading and preprocessing (if needed)"],"metadata":{"id":"YJkQ13rjmeNX"}},{"cell_type":"code","source":["# Method for building a data sample from dataset file\n","def build_sample(s, label_dict, source_lang, vocabs):\n","\n","    # we need to build the sentence that is formed by individual word elements\n","    sentence = ''\n","    # stored if partial processing data is needed due to resource limits\n","    sentence_id = s.attrib['id']\n","    words = []\n","    for word in s:\n","        text = str(word.text).replace('_', ' ')\n","        sentence += f'{text} '\n","        # if the word is a target word to translate and disambiguate\n","        if word.tag == 'instance':\n","            pos = word.attrib['pos']\n","            id = word.attrib['id']\n","            labels = label_dict[id]\n","            # get all possible translations\n","            translations = {}\n","            for lang in vocabs.keys():\n","                if id in vocabs[lang].keys():\n","                    translations[lang] = vocabs[lang][id]\n","            words.append(Word(text, pos, translations, labels))\n","    sample = WsdSample(sentence_id, sentence, words)\n","\n","    return sample\n","\n","def process_data(sentence_path, label_path, source_lang, target_langs = TARGET_LANGS):\n","    print('Processing data...')\n","    # to avoid too many computations (before was requests to BabelNet) at the first try the samples object are stored using pickle and then loaded again\n","    samples_path = f'samples/samples_{source_lang}.pkl'\n","    vocabs = build_vocabs(source_lang)\n","\n","    # If data is already preprocessed, load the list of samples and check if it is complete or incomplete due to resource limits\n","    if os.path.exists(samples_path):\n","        print('Data found, loading and checking if it is complete..')\n","        samples = []\n","        with open(samples_path, 'rb') as f:\n","            try:\n","                while True:\n","                    samples.append(pickle.load(f))\n","            except EOFError:\n","                pass\n","\n","        # retrieve last sample and check if the processing is complete, if not finish the job\n","        last_sample_id = samples[-1].id\n","        checkpoint = False\n","        label_dict = {}\n","        with open(label_path, 'r') as f:\n","            lines = f.readlines()\n","            for line in lines:\n","                splitted = line.split(' ')\n","                label_dict[splitted[0]] = []\n","                for label in splitted[1:]:\n","                    if label.endswith('\\n'):\n","                        label = label[:-1]\n","                    label_dict[splitted[0]].append(label)\n","\n","        xml = ET.parse(sentence_path)\n","        root = xml.getroot()\n","        for t in root:\n","            for s in tqdm(t, desc='Checking'):\n","                if last_sample_id == s.attrib['id']:\n","                    checkpoint = True\n","                elif checkpoint:\n","                    sample = build_sample(s, label_dict, source_lang, vocabs)\n","                    samples.append(sample)\n","                    pickle.dump(sample, open(samples_path, 'ab+'))\n","    # if it is the first time to process data\n","    else:\n","        label_dict = {}\n","        with open(label_path, 'r') as f:\n","            lines = f.readlines()\n","            for line in lines:\n","                splitted = line.split(' ')\n","                label_dict[splitted[0]] = []\n","                for label in splitted[1:]:\n","                    if label.endswith('\\n'):\n","                        label = label[:-1]\n","                    label_dict[splitted[0]].append(label)\n","        samples = []\n","        # parsing xml file and building list of WsdSample objects\n","        xml = ET.parse(sentence_path)\n","        root = xml.getroot()\n","        for t in root:\n","            for s in tqdm(t, desc='Building samples'):\n","                sample = build_sample(s, label_dict, source_lang, vocabs)\n","                samples.append(sample)\n","                pickle.dump(sample, open(samples_path, 'ab+'))\n","\n","    return samples, vocabs"],"metadata":{"id":"vnYic_F1mzX3","execution":{"iopub.status.busy":"2024-03-18T15:28:32.404499Z","iopub.execute_input":"2024-03-18T15:28:32.404879Z","iopub.status.idle":"2024-03-18T15:28:32.426172Z","shell.execute_reply.started":"2024-03-18T15:28:32.404846Z","shell.execute_reply":"2024-03-18T15:28:32.425471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Translation function\n","Needed for both dataset and example runs"],"metadata":{"id":"CRtCebXW5Jn3"}},{"cell_type":"code","source":["# Main method for translating a single word, using context sentence and PLM and storing the probabilities of all possible translations\n","def translate(sentence, word : Word, tokenizer, model, target_lang, paper = False):\n","    # create the right prompt to feed to the model\n","    prompt = get_prompt(sentence, word.text, target_lang)\n","    prompt_tokens = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')['input_ids'].requires_grad_(False).to(device)\n","    # call the model to generate next token\n","    with torch.no_grad():\n","        output = model(prompt_tokens, use_cache = True)\n","    # storing cache for next call of the model, to avoid using again the whole sentence\n","    cache = output['past_key_values']\n","    logits = output['logits'] #[batch_size, len_seq, vocab_size]\n","    # removing 1st dimension i.e. batch_size, here = 1, and then selecting just the last output\n","    logits = logits.squeeze()[-1,:]\n","    # softmax to obtain probabilities\n","    vocab_prob = F.log_softmax(logits, dim=-1)\n","    result_dict = {}\n","    # use the paper method, feeding the model sequentially with all the tokens of a word and compute its average probability\n","    if paper:\n","        for w in word.translations[target_lang]:\n","            token_probs = []\n","            tokens = tokenizer(w, add_special_tokens=False)['input_ids'][0][0]\n","            # each token is stored its probability and then it is passed to the model with the previous cache\n","            for i, token in enumerate(tokens):\n","                token_probs.append(vocab_prob[token])\n","                if i+1 < len(tokens):\n","                    input = torch.LongTensor([[token]]).to(device)#, requires_grad=False).to(device)\n","                    output = model(input, past_key_values=cache, use_cache=True)\n","                    cache = output['past_key_values']\n","                    logits = output['logits'].squeeze()\n","                    vocab_prob = F.log_softmax(logits, dim=-1)\n","                else:\n","                    result_dict[w] = sum(token_probs)/len(token_probs)\n","    else:\n","        # given that in the paper method the iterations after the first one are biased, and the final probabilities\n","        # could be too optimistic, here we take just the probabilities of the first token. If there are words with the same first token\n","        # compute also the next one and do the average as the paper method.\n","        tokens = tokenizer(word.translations[target_lang], add_special_tokens=False)['input_ids']\n","        mask = get_tokens_mask(tokens)\n","        for i, w in enumerate(tokens):\n","            # if the word has its first token as unique\n","            if len(mask[i]) == 0:\n","                result_dict[word.translations[target_lang][i]] = vocab_prob[w[0]]\n","            # if to disambiguate its results we need to take into consideration other tokens\n","            else:\n","                token_probs = []\n","                token_probs.append(vocab_prob[w[0]])\n","                for j, token in enumerate(w):\n","                    if len(mask[i]) < j:\n","                        input = torch.LongTensor([[token]]).to(device)#, requires_grad = False).to(device)\n","                        output = model(input, past_key_values=cache, use_cache=True)\n","                        cache = output['past_key_values']\n","                        logits = output['logits'].squeeze()\n","                        prob = F.log_softmax(logits, dim=-1)\n","                        token_probs.append(prob[token])\n","                result_dict[word.translations[target_lang][i]] = sum(token_probs)/len(token_probs)\n","    return result_dict"],"metadata":{"id":"f0QenIYH5dzV","execution":{"iopub.status.busy":"2024-03-18T15:28:32.427650Z","iopub.execute_input":"2024-03-18T15:28:32.427930Z","iopub.status.idle":"2024-03-18T15:28:32.448208Z","shell.execute_reply.started":"2024-03-18T15:28:32.427907Z","shell.execute_reply":"2024-03-18T15:28:32.447290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Run and evaluation\n","The last cell actually runs the code, with default options of BLOOM 3B as model and Italian as source language.\n","\n","Furthermore, in the last cell again, some more possible choices can be found as comments regarding different models and different source languages. If you want to try other languages please read instructions at the top of the notebook.\n","\n","Output with results and metrics can be found in the output/ folder."],"metadata":{"id":"LXD4eQYenZs2"}},{"cell_type":"code","source":["# Processing a dataset sentence with all of its target words and returning the best results for each target language\n","def process_sample(sample : WsdSample, model, tokenizer, print_results=False):\n","    sample_results = []\n","    for word in tqdm(sample.words, desc='Words', leave=False):\n","        single_word_results = {}\n","        for lang in word.translations.keys():\n","            word_results = translate(sample.sentence, word, tokenizer, model, target_lang=lang) # add paper=True to use the paper method\n","            best_result = -9999\n","            best_words = []\n","            for key in word_results:\n","                word_results[key] = truncate(word_results[key], 3)\n","                if word_results[key] > best_result:\n","                    best_result = word_results[key]\n","                    best_words = [key]\n","                if word_results[key] == best_result:\n","                    best_words.append(key)\n","            single_word_results[lang] = (best_words, word.pos, best_result, word_results)\n","        sample_results.append(single_word_results)\n","        if print_results:\n","            print('Results for word ', word.text, ': ')\n","            print(sample_results)\n","    # ritorno il miglior risultato per ogni lingua\n","    return sample.words, sample_results # is a List[dict] where for each word 'language' : (best_translations, pos, score, other results dict)"],"metadata":{"id":"Ft8We8QungGF","execution":{"iopub.status.busy":"2024-03-18T15:28:32.449426Z","iopub.execute_input":"2024-03-18T15:28:32.449812Z","iopub.status.idle":"2024-03-18T15:28:32.465599Z","shell.execute_reply.started":"2024-03-18T15:28:32.449781Z","shell.execute_reply":"2024-03-18T15:28:32.464633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Method for computing results over a given language dataset\n","def wsd_on_dataset(model_name = 'bigscience/bloom-3b', source_lang = 'it', target_langs = TARGET_LANGS):\n","\n","    if source_lang in target_langs:\n","        target_langs.remove(source_lang)\n","\n","    # uploading model and its tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, return_dict_in_generate=True, torch_dtype='auto', device_map='auto')\n","\n","    # computing paths\n","    print('Computing paths...')\n","    sentence_path, label_path = data_paths(source_lang)\n","    output_path = f'output/{source_lang}_translations.txt'\n","    metrics_path = f'output/{source_lang}_metrics.txt'\n","    # preprocess data\n","    samples, _ = process_data(sentence_path, label_path, source_lang, target_langs)\n","    ids = build_id_dicts(source_lang)\n","    print('Data Completed..')\n","\n","    # vars for metrics\n","    correct = 0\n","    wrong = 0\n","    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n","    total_words = 0\n","    jaccard_index = 0\n","\n","    print('WSD started')\n","    for sample in tqdm(samples, desc='Dataset progress'):\n","        words, results = process_sample(sample, model, tokenizer)\n","        with open(output_path, 'a', encoding=\"utf-8\") as f:\n","            f.write(f'Sentence: {sample.sentence}\\nWords:\\n')\n","            for i, d in enumerate(results):\n","                total_words += 1\n","                synsets = {}\n","                f.write(f'{words[i].text} :\\n')\n","                # use the same iteration to retrieve the most promising synsets\n","                for lang in d.keys():\n","                    best_words, pos, best_score, scores = d[lang]\n","                    f.write(f'\\t{lang}: best translation: {best_words}, ')\n","                    f.write(f', score = {best_score}\\n')\n","                    f.write(f'\\tAll scores: ')\n","                    for j, (k,v) in enumerate(scores.items()):\n","                        f.write(f'[{k}: {v}] ')\n","                    f.write(f'\\n')\n","                    # assign multiplicity\n","                    for best in best_words:\n","                        if best in ids[lang].keys():\n","                            for id in ids[lang][best]:\n","                                if id not in synsets.keys():\n","                                    synsets[id] = 1\n","                                else:\n","                                    synsets[id] += 1\n","                max_value = max(synsets.values())\n","                # select best synset ids\n","                best_ids = []\n","                for id in synsets.keys():\n","                    if synsets[id] == max_value:\n","                        best_ids.append(id)\n","                # temporal values for each word results\n","                tp, fp, tn, fn = 0, 0, 0, 0\n","                found = False\n","                f.write(f'\\tPredicted ids: {best_ids}\\n')\n","                f.write(f'\\tLabels: {words[i].labels}\\n')\n","                for id in best_ids:\n","                    if id in words[i].labels:\n","                        tp += 1\n","                        if not found:\n","                            correct += 1\n","                            found = True\n","                    else:\n","                        fp += 1\n","                fn = len(words[i].labels) - tp\n","                tn = len(synsets.keys()) - (tp + fp)\n","                jaccard_index += tp / (tp + fp + fn)\n","                true_positive += tp\n","                true_negative += tn\n","                false_positive += fp\n","                false_negative += fn\n","                if not found:\n","                    wrong += 1\n","        temp_accuracy = (correct/(correct+wrong))*100\n","        temp_recall = true_positive / (true_positive + false_negative + 1e-10)\n","        temp_precision = true_positive / (true_positive + false_positive + 1e-10)\n","        temp_f1 = 2 * ((temp_precision * temp_recall) / (temp_precision + temp_recall + 1e-10))\n","        temp_jaccard_index = jaccard_index / total_words\n","        with open(metrics_path, 'w') as f:\n","            f.write(f'Accuracy percentage = {temp_accuracy}%\\n')\n","            f.write(f'Recall = {temp_recall}\\n')\n","            f.write(f'Precision = {temp_precision}\\n')\n","            f.write(f'F1 Score = {temp_f1}\\n')\n","            f.write(f'Jaccard Index = {temp_jaccard_index}\\n')\n","            f.write(f'Correct: {correct}\\n')\n","            f.write(f'Wrong: {wrong}')\n","    accuracy = (correct/(correct+wrong))*100\n","    recall = true_positive / (true_positive + false_negative + 1e-10)\n","    precision = true_positive / (true_positive + false_positive + 1e-10)\n","    f1 = 2 * ((precision * recall) / (precision + recall + 1e-10))\n","    jaccard_index = jaccard_index / total_words\n","    with open(metrics_path, 'w') as f:\n","        f.write(f'Accuracy percentage = {accuracy}%\\n')\n","        f.write(f'Recall = {recall}\\n')\n","        f.write(f'Precision = {precision}\\n')\n","        f.write(f'F1 Score = {f1}\\n')\n","        f.write(f'Jaccard Index = {jaccard_index}')\n","        f.write(f'Correct: {correct}\\n')\n","        f.write(f'Wrong: {wrong}')\n","        f.write(f'True Positives: {true_positive}')\n","        f.write(f'True Negatives: {true_negative}')\n","        f.write(f'False Positives: {false_positive}')\n","        f.write(f'False Negatives: {false_negative}')"],"metadata":{"id":"akXiMtvJn5Vw","execution":{"iopub.status.busy":"2024-03-18T15:28:32.466782Z","iopub.execute_input":"2024-03-18T15:28:32.467050Z","iopub.status.idle":"2024-03-18T15:28:32.492228Z","shell.execute_reply.started":"2024-03-18T15:28:32.467027Z","shell.execute_reply":"2024-03-18T15:28:32.491363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# other models: \"bigscience/bloom-560m\" \"bigscience/bloom-1b7\" \"bigscience/bloom-1b1\" \"bigscience/bloom-3b\" \"bigscience/bloom-7b1\"\n","# other source languages: 'it' 'fr' 'es' 'en'\n","wsd_on_dataset('bigscience/bloom-3b')"],"metadata":{"id":"HqGwHsncAe-s","execution":{"iopub.status.busy":"2024-03-18T15:28:32.493743Z","iopub.execute_input":"2024-03-18T15:28:32.494060Z","iopub.status.idle":"2024-03-18T16:06:44.758988Z","shell.execute_reply.started":"2024-03-18T15:28:32.494035Z","shell.execute_reply":"2024-03-18T16:06:44.758084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example Run\n","This is just an example, you can specify a sentence, a word contained in the sentence, its language and the output will be all the best translations in the three target languages, their scores and the final possible meanings for that words. Also it saves the same result in a file in the examples/ folder.\n","\n","It actually uses BabelNet, because doing so you can specify whatever target word you want, without relying on preprocessed data.\n","\n","But in order to run it, you should download this notebook and running it locally, it takes around 1 minute with CPU."],"metadata":{"id":"DAbi7TvkzJyB"}},{"cell_type":"code","source":["import babelnet as bn\n","from babelnet import Language, POS\n","from babelnet.data.lemma import BabelLemmaType\n","\n","# Useful for single example run\n","BABEL_LANG = {\n","    'en' : Language.EN,\n","    'it' : Language.IT,\n","    'es' : Language.ES,\n","    'fr' : Language.FR,\n","    'ru' : Language.RU,\n","    'zh' : Language.ZH\n","}\n","\n","# SINGLE DISAMBIGUATION EXAMPLE\n","# given a sentence, a word, a source language, a target language and a model\n","# translates it and computes its senses\n","def disambiguate(sentence, word, source_lang, target_langs = TARGET_LANGS, model_name = 'bigscience/bloom-3b'):\n","\n","    if source_lang in target_langs:\n","        target_langs.remove(source_lang)\n","\n","    # uploading model and its tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, return_dict_in_generate=True)#, torch_dtype='auto', device_map='auto')\n","\n","    # creating Word object\n","    print('Retrieving all possible translations..')\n","    translations = {}\n","    for lang in target_langs:\n","        translations[lang] = []\n","        babel_synsets = bn.get_synsets(word, from_langs=[BABEL_LANG[source_lang]], to_langs=[BABEL_LANG[lang]])\n","        for s in babel_synsets:\n","            lemma = s.lemmas(BABEL_LANG[lang], BabelLemmaType.HIGH_QUALITY)\n","            if len(lemma) > 0:\n","                translations[lang].append(str(lemma[0]).replace('_', ' '))\n","    target_word = Word(word, translations=translations)\n","\n","    disambiguations = {}\n","    ids = {}\n","\n","    for lang in target_langs:\n","        results = translate(sentence, target_word, tokenizer, model, lang)\n","        best_result = -9999\n","        best_words = []\n","        for key in results:\n","            results[key] = truncate(results[key], 3)\n","            if results[key] > best_result:\n","                best_result = results[key]\n","                best_words = [key]\n","            if results[key] == best_result:\n","                best_words.append(key)\n","        for word in best_words:\n","            synsets = bn.get_synsets(word, from_langs=[BABEL_LANG[lang]])\n","            for s in synsets:\n","                if s.id in ids.keys():\n","                    ids[s.id] += 1\n","                else:\n","                    ids[s.id] = 1\n","        disambiguations[lang] = (best_words, best_result)\n","\n","    max_value = max(ids.values())\n","    best_ids = []\n","    for id in ids.keys():\n","        if ids[id] == max_value:\n","            best_ids.append(id)\n","\n","    glosses = []\n","    for id in best_ids:\n","        synsets = bn.get_synsets(id, from_langs=[BABEL_LANG[source_lang]])\n","        for s in synsets:\n","            glosses.append(str(s.main_gloss(BABEL_LANG[source_lang])))\n","\n","    return disambiguations, glosses\n","\n","# EXAMPLE FOR A SINGLE WORD TO TRANSLATE/DISAMBIGUATE\n","# Some of the glosses returned can be wrong, but at least one is true. This is because once the word is translated (even correctly)\n","# that word can also have more than one meaning in the target languages and be ambiguous too.\n","def example(sentence = 'Spero che questo progetto mi far√† prendere un voto molto alto.',\n","            target_word = 'progetto', source_lang = 'it', model_name = 'bigscience/bloom-3b'):\n","    disambiguations, glosses = disambiguate(sentence, target_word, source_lang, model_name=model_name)\n","    #text_target_language = LANG_DICT[target_lang]\n","    with open('examples/example.txt', 'w', encoding='utf-8') as f:\n","        f.write(f'The word \\'{target_word}\\' in the sentence \\'{sentence}\\'\\n')\n","        print(f'The word \\'{target_word}\\' in the sentence \\'{sentence}\\'\\n')\n","        for lang in disambiguations:\n","            words, scores = disambiguations[lang]\n","            f.write(f'Has been translated in {LANG_DICT[lang]} as {words} with {scores}\\n')\n","            print(f'Has been translated in {LANG_DICT[lang]} as {words} with {scores}\\n')\n","        f.write(f'And through these translations the inferred meanings are: {glosses}\\n')\n","        print(f'And through these translations the inferred meanings are: {glosses}\\n')"],"metadata":{"id":"u7vnb68QzL6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example()"],"metadata":{"id":"OGSen84CzNmS"},"execution_count":null,"outputs":[]}]}